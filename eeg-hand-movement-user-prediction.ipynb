{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":757973,"sourceType":"datasetVersion","datasetId":391999}],"dockerImageVersionId":30042,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Hand Movement and User Prediction  \n\nGiven *data EEG data from different users performing different hand movements*, let's try to predict the **hand movement** and the **user** of a given reading.\n\nWe will use a TensorFlow neural network to make our predictions. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-02-23T21:32:07.152855Z","iopub.execute_input":"2024-02-23T21:32:07.153358Z","iopub.status.idle":"2024-02-23T21:32:13.761625Z","shell.execute_reply.started":"2024-02-23T21:32:07.153313Z","shell.execute_reply":"2024-02-23T21:32:13.760844Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dfs = [pd.read_csv('../input/eeg-data-from-hands-movement/Dataset/user_' + user + '.csv') for user in ['a', 'b', 'c', 'd']]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:13.763166Z","iopub.execute_input":"2024-02-23T21:32:13.763438Z","iopub.status.idle":"2024-02-23T21:32:14.261726Z","shell.execute_reply.started":"2024-02-23T21:32:13.763409Z","shell.execute_reply":"2024-02-23T21:32:14.260682Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for i in range(len(dfs)):\n    dfs[i]['User'] = pd.Series(i, index=dfs[i].index)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.264239Z","iopub.execute_input":"2024-02-23T21:32:14.264669Z","iopub.status.idle":"2024-02-23T21:32:14.279814Z","shell.execute_reply.started":"2024-02-23T21:32:14.264624Z","shell.execute_reply":"2024-02-23T21:32:14.278783Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = pd.concat(dfs, axis=0).sample(frac=1.0, random_state=123).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.281849Z","iopub.execute_input":"2024-02-23T21:32:14.282176Z","iopub.status.idle":"2024-02-23T21:32:14.305718Z","shell.execute_reply.started":"2024-02-23T21:32:14.282148Z","shell.execute_reply":"2024-02-23T21:32:14.304934Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.308965Z","iopub.execute_input":"2024-02-23T21:32:14.309316Z","iopub.status.idle":"2024-02-23T21:32:14.348648Z","shell.execute_reply.started":"2024-02-23T21:32:14.309279Z","shell.execute_reply":"2024-02-23T21:32:14.347677Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       Class  AF3 delta std  AF3 delta m  AF3 theta std  AF3 theta m  \\\n0        0.0    3572.252440  2065.056469       0.851824     2.047953   \n1        1.0    3574.116024  2065.528155       1.932513     3.321636   \n2        0.0    3554.487593  2056.215665       0.935015     3.793783   \n3        0.0    3570.668125  2063.974908       1.875394     3.028541   \n4        0.0    3559.747108  2057.401763       1.053691     1.530594   \n...      ...            ...          ...            ...          ...   \n11515    2.0    3570.748191  2063.201099       0.727594     1.404708   \n11516    1.0    3566.600695  2062.436502       1.491797     3.013466   \n11517    0.0    3574.186933  2064.477869       0.710708     1.534898   \n11518    0.0    3574.343116  2065.996679       0.606401     2.835015   \n11519    1.0    3571.646152  2064.292147       0.657439     2.576671   \n\n       AF3 alpha std  AF3 alpha m  AF3 beta std  AF3 beta m  F7 delta std  \\\n0           0.651871     2.522036      2.109733    3.347705   3564.779879   \n1           1.138012     2.349805      2.256212    3.945981   3563.399422   \n2           0.736168     2.680542      3.381325    4.678876   3568.839949   \n3           0.910000     3.018672      1.163312    2.883009   3565.403408   \n4           1.593121     2.789907      2.668865    4.326693   3573.651774   \n...              ...          ...           ...         ...           ...   \n11515       0.995674     2.569811      0.991322    1.987807   3562.594707   \n11516       1.338308     2.769207      4.268401    3.877338   3576.187723   \n11517       0.749206     2.038258      2.208018    2.157076   3566.398415   \n11518       0.617079     1.209722      3.357912    3.098423   3566.521533   \n11519       0.867987     3.159366      1.204885    2.930878   3563.574264   \n\n       ...  F8 beta m  AF4 delta std  AF4 delta m  AF4 theta std  AF4 theta m  \\\n0      ...  35.151586    3628.426885  2129.789645       5.353671    17.885132   \n1      ...  40.800889    3680.341349  2144.200503      10.819521    36.995982   \n2      ...  18.176841    3538.347368  2081.315814       5.486555    13.204753   \n3      ...  55.547547    3604.601528  2122.493834      15.611283    16.452483   \n4      ...   7.208052    3513.244789  2030.461207       1.455450     3.030659   \n...    ...        ...            ...          ...            ...          ...   \n11515  ...   1.910036    3567.687654  2062.791757       1.748982     2.097780   \n11516  ...   5.217772    3548.277991  2055.537892       1.787661     3.366967   \n11517  ...   2.200203    3572.290285  2063.776063       1.268569     2.547969   \n11518  ...  15.413625    3608.394990  2091.509508       1.479338     3.824176   \n11519  ...   2.347369    3570.863440  2064.080620       1.406384     2.024807   \n\n       AF4 alpha std  AF4 alpha m  AF4 beta std  AF4 beta m  User  \n0           7.672209    29.960618     43.216980   43.932669     0  \n1          12.812193    24.146774     23.747501   49.072017     0  \n2           0.664075     6.633072      1.434277    4.132446     2  \n3          22.462175    46.703612     32.213578   70.892466     0  \n4           0.482971     3.200647      0.895170    2.099638     2  \n...              ...          ...           ...         ...   ...  \n11515       1.242056     1.682180      1.277379    1.997654     3  \n11516       0.810903     1.739812      2.202195    3.294145     2  \n11517       0.887808     1.651011      1.533523    1.957866     1  \n11518       3.281520     4.238889     33.441194   19.158094     0  \n11519       0.758809     2.726751      1.396227    2.646152     1  \n\n[11520 rows x 114 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>AF3 delta std</th>\n      <th>AF3 delta m</th>\n      <th>AF3 theta std</th>\n      <th>AF3 theta m</th>\n      <th>AF3 alpha std</th>\n      <th>AF3 alpha m</th>\n      <th>AF3 beta std</th>\n      <th>AF3 beta m</th>\n      <th>F7 delta std</th>\n      <th>...</th>\n      <th>F8 beta m</th>\n      <th>AF4 delta std</th>\n      <th>AF4 delta m</th>\n      <th>AF4 theta std</th>\n      <th>AF4 theta m</th>\n      <th>AF4 alpha std</th>\n      <th>AF4 alpha m</th>\n      <th>AF4 beta std</th>\n      <th>AF4 beta m</th>\n      <th>User</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>3572.252440</td>\n      <td>2065.056469</td>\n      <td>0.851824</td>\n      <td>2.047953</td>\n      <td>0.651871</td>\n      <td>2.522036</td>\n      <td>2.109733</td>\n      <td>3.347705</td>\n      <td>3564.779879</td>\n      <td>...</td>\n      <td>35.151586</td>\n      <td>3628.426885</td>\n      <td>2129.789645</td>\n      <td>5.353671</td>\n      <td>17.885132</td>\n      <td>7.672209</td>\n      <td>29.960618</td>\n      <td>43.216980</td>\n      <td>43.932669</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>3574.116024</td>\n      <td>2065.528155</td>\n      <td>1.932513</td>\n      <td>3.321636</td>\n      <td>1.138012</td>\n      <td>2.349805</td>\n      <td>2.256212</td>\n      <td>3.945981</td>\n      <td>3563.399422</td>\n      <td>...</td>\n      <td>40.800889</td>\n      <td>3680.341349</td>\n      <td>2144.200503</td>\n      <td>10.819521</td>\n      <td>36.995982</td>\n      <td>12.812193</td>\n      <td>24.146774</td>\n      <td>23.747501</td>\n      <td>49.072017</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>3554.487593</td>\n      <td>2056.215665</td>\n      <td>0.935015</td>\n      <td>3.793783</td>\n      <td>0.736168</td>\n      <td>2.680542</td>\n      <td>3.381325</td>\n      <td>4.678876</td>\n      <td>3568.839949</td>\n      <td>...</td>\n      <td>18.176841</td>\n      <td>3538.347368</td>\n      <td>2081.315814</td>\n      <td>5.486555</td>\n      <td>13.204753</td>\n      <td>0.664075</td>\n      <td>6.633072</td>\n      <td>1.434277</td>\n      <td>4.132446</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>3570.668125</td>\n      <td>2063.974908</td>\n      <td>1.875394</td>\n      <td>3.028541</td>\n      <td>0.910000</td>\n      <td>3.018672</td>\n      <td>1.163312</td>\n      <td>2.883009</td>\n      <td>3565.403408</td>\n      <td>...</td>\n      <td>55.547547</td>\n      <td>3604.601528</td>\n      <td>2122.493834</td>\n      <td>15.611283</td>\n      <td>16.452483</td>\n      <td>22.462175</td>\n      <td>46.703612</td>\n      <td>32.213578</td>\n      <td>70.892466</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>3559.747108</td>\n      <td>2057.401763</td>\n      <td>1.053691</td>\n      <td>1.530594</td>\n      <td>1.593121</td>\n      <td>2.789907</td>\n      <td>2.668865</td>\n      <td>4.326693</td>\n      <td>3573.651774</td>\n      <td>...</td>\n      <td>7.208052</td>\n      <td>3513.244789</td>\n      <td>2030.461207</td>\n      <td>1.455450</td>\n      <td>3.030659</td>\n      <td>0.482971</td>\n      <td>3.200647</td>\n      <td>0.895170</td>\n      <td>2.099638</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11515</th>\n      <td>2.0</td>\n      <td>3570.748191</td>\n      <td>2063.201099</td>\n      <td>0.727594</td>\n      <td>1.404708</td>\n      <td>0.995674</td>\n      <td>2.569811</td>\n      <td>0.991322</td>\n      <td>1.987807</td>\n      <td>3562.594707</td>\n      <td>...</td>\n      <td>1.910036</td>\n      <td>3567.687654</td>\n      <td>2062.791757</td>\n      <td>1.748982</td>\n      <td>2.097780</td>\n      <td>1.242056</td>\n      <td>1.682180</td>\n      <td>1.277379</td>\n      <td>1.997654</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>11516</th>\n      <td>1.0</td>\n      <td>3566.600695</td>\n      <td>2062.436502</td>\n      <td>1.491797</td>\n      <td>3.013466</td>\n      <td>1.338308</td>\n      <td>2.769207</td>\n      <td>4.268401</td>\n      <td>3.877338</td>\n      <td>3576.187723</td>\n      <td>...</td>\n      <td>5.217772</td>\n      <td>3548.277991</td>\n      <td>2055.537892</td>\n      <td>1.787661</td>\n      <td>3.366967</td>\n      <td>0.810903</td>\n      <td>1.739812</td>\n      <td>2.202195</td>\n      <td>3.294145</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11517</th>\n      <td>0.0</td>\n      <td>3574.186933</td>\n      <td>2064.477869</td>\n      <td>0.710708</td>\n      <td>1.534898</td>\n      <td>0.749206</td>\n      <td>2.038258</td>\n      <td>2.208018</td>\n      <td>2.157076</td>\n      <td>3566.398415</td>\n      <td>...</td>\n      <td>2.200203</td>\n      <td>3572.290285</td>\n      <td>2063.776063</td>\n      <td>1.268569</td>\n      <td>2.547969</td>\n      <td>0.887808</td>\n      <td>1.651011</td>\n      <td>1.533523</td>\n      <td>1.957866</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11518</th>\n      <td>0.0</td>\n      <td>3574.343116</td>\n      <td>2065.996679</td>\n      <td>0.606401</td>\n      <td>2.835015</td>\n      <td>0.617079</td>\n      <td>1.209722</td>\n      <td>3.357912</td>\n      <td>3.098423</td>\n      <td>3566.521533</td>\n      <td>...</td>\n      <td>15.413625</td>\n      <td>3608.394990</td>\n      <td>2091.509508</td>\n      <td>1.479338</td>\n      <td>3.824176</td>\n      <td>3.281520</td>\n      <td>4.238889</td>\n      <td>33.441194</td>\n      <td>19.158094</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11519</th>\n      <td>1.0</td>\n      <td>3571.646152</td>\n      <td>2064.292147</td>\n      <td>0.657439</td>\n      <td>2.576671</td>\n      <td>0.867987</td>\n      <td>3.159366</td>\n      <td>1.204885</td>\n      <td>2.930878</td>\n      <td>3563.574264</td>\n      <td>...</td>\n      <td>2.347369</td>\n      <td>3570.863440</td>\n      <td>2064.080620</td>\n      <td>1.406384</td>\n      <td>2.024807</td>\n      <td>0.758809</td>\n      <td>2.726751</td>\n      <td>1.396227</td>\n      <td>2.646152</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>11520 rows × 114 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def onehot_encode(df, column):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=column)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:41:46.650842Z","iopub.execute_input":"2024-02-23T21:41:46.651225Z","iopub.status.idle":"2024-02-23T21:41:46.657329Z","shell.execute_reply.started":"2024-02-23T21:41:46.651192Z","shell.execute_reply":"2024-02-23T21:41:46.656037Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def preprocess_inputs(df, target='Class'):\n    df = df.copy()\n    \n    # One-hot encode whichever target column is not being used\n    targets = ['Class', 'User']\n    targets.remove(target)\n    df = onehot_encode(df, column=targets[0])\n    \n    # Split df into X and y\n    y = df[target].copy()\n    X = df.drop(target, axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n    \n    # Scale X with a standard scaler\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    \n    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n    \n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.402823Z","iopub.execute_input":"2024-02-23T21:32:14.403051Z","iopub.status.idle":"2024-02-23T21:32:14.410249Z","shell.execute_reply.started":"2024-02-23T21:32:14.403021Z","shell.execute_reply":"2024-02-23T21:32:14.409554Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def build_model(num_classes=3):\n    \n    inputs = tf.keras.Input(shape=(X_train.shape[1],))\n    x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.411061Z","iopub.execute_input":"2024-02-23T21:32:14.411366Z","iopub.status.idle":"2024-02-23T21:32:14.422211Z","shell.execute_reply.started":"2024-02-23T21:32:14.411344Z","shell.execute_reply":"2024-02-23T21:32:14.421470Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Hand Movement Class","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = preprocess_inputs(data, target='Class')","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.423375Z","iopub.execute_input":"2024-02-23T21:32:14.423680Z","iopub.status.idle":"2024-02-23T21:32:14.484233Z","shell.execute_reply.started":"2024-02-23T21:32:14.423647Z","shell.execute_reply":"2024-02-23T21:32:14.483632Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.485147Z","iopub.execute_input":"2024-02-23T21:32:14.485489Z","iopub.status.idle":"2024-02-23T21:32:14.513900Z","shell.execute_reply.started":"2024-02-23T21:32:14.485465Z","shell.execute_reply":"2024-02-23T21:32:14.513116Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      AF3 delta std  AF3 delta m  AF3 theta std  AF3 theta m  AF3 alpha std  \\\n0         -0.592492    -0.628348      -0.961159     0.055905       0.463966   \n1         -1.215542    -1.060438      -0.687275     0.911497       0.266495   \n2          0.217034     0.061461      -0.302453    -0.276674      -0.609120   \n3          2.636827     3.089887       0.715433     1.688836       1.115520   \n4         -0.012351     0.039718      -0.098249    -0.213172       0.948769   \n...             ...          ...            ...          ...            ...   \n8058       0.618517     1.150023      -0.957440    -0.735247       0.634060   \n8059      -0.157839    -0.326705      -0.099050     0.460753      -0.022372   \n8060       0.166508     0.010035       0.018053    -0.268545      -1.090114   \n8061       0.202509    -0.017317      -0.267921    -0.589426       0.478543   \n8062      -0.271889    -0.369114      -0.112442     0.388119      -0.873026   \n\n      AF3 alpha m  AF3 beta std  AF3 beta m  F7 delta std  F7 delta m  ...  \\\n0        0.243742     -1.048130   -0.492935     -1.039009   -0.724031  ...   \n1       -0.045726     -0.996263   -1.073110     -1.595935   -1.789505  ...   \n2       -0.737363     -1.144599   -1.598413     -1.092270   -1.223951  ...   \n3        0.633049     -0.753087   -0.640998      0.643631    0.970393  ...   \n4       -0.058219     -0.254097   -0.487547      0.332959    0.028684  ...   \n...           ...           ...         ...           ...         ...  ...   \n8058     1.144167      1.837545    2.507360      1.076999    1.533723  ...   \n8059    -0.179817      1.098608    0.432161     -0.522874   -0.840956  ...   \n8060     0.455020      0.757639    1.987062      0.999285    0.692423  ...   \n8061    -0.449163     -0.196672   -0.763750     -0.557585   -0.719922  ...   \n8062     0.282208      0.973119    1.484744      0.914695    1.269811  ...   \n\n      AF4 theta std  AF4 theta m  AF4 alpha std  AF4 alpha m  AF4 beta std  \\\n0         -0.436216    -0.408949      -0.407029    -0.490410     -0.536367   \n1          2.577270     2.173543       3.159511     3.022871      0.657025   \n2         -0.400908    -0.385079      -0.267008    -0.348359     -0.541364   \n3          0.141047    -0.008948      -0.427868    -0.301202     -0.505539   \n4         -0.565201    -0.479292      -0.372068    -0.497185     -0.523806   \n...             ...          ...            ...          ...           ...   \n8058      -0.278668     0.153400      -0.380571    -0.152910     -0.500346   \n8059       2.779410     1.612038       1.694883     4.498588      3.159293   \n8060      -0.430248    -0.416324      -0.096056    -0.372181     -0.508712   \n8061      -0.238893    -0.510057      -0.441859    -0.499372     -0.511246   \n8062       0.145982    -0.239209      -0.348275    -0.378249     -0.550461   \n\n      AF4 beta m    User_0    User_1    User_2    User_3  \n0      -0.539566 -0.574343 -0.578735 -0.575870  1.722792  \n1       2.073117  1.741120 -0.578735 -0.575870 -0.580453  \n2      -0.569416 -0.574343 -0.578735 -0.575870  1.722792  \n3      -0.522675 -0.574343 -0.578735 -0.575870  1.722792  \n4      -0.525149 -0.574343  1.727908 -0.575870 -0.580453  \n...          ...       ...       ...       ...       ...  \n8058   -0.453114 -0.574343 -0.578735  1.736502 -0.580453  \n8059    2.567176  1.741120 -0.578735 -0.575870 -0.580453  \n8060   -0.493789 -0.574343 -0.578735  1.736502 -0.580453  \n8061   -0.538075 -0.574343  1.727908 -0.575870 -0.580453  \n8062   -0.559248 -0.574343 -0.578735  1.736502 -0.580453  \n\n[8063 rows x 116 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AF3 delta std</th>\n      <th>AF3 delta m</th>\n      <th>AF3 theta std</th>\n      <th>AF3 theta m</th>\n      <th>AF3 alpha std</th>\n      <th>AF3 alpha m</th>\n      <th>AF3 beta std</th>\n      <th>AF3 beta m</th>\n      <th>F7 delta std</th>\n      <th>F7 delta m</th>\n      <th>...</th>\n      <th>AF4 theta std</th>\n      <th>AF4 theta m</th>\n      <th>AF4 alpha std</th>\n      <th>AF4 alpha m</th>\n      <th>AF4 beta std</th>\n      <th>AF4 beta m</th>\n      <th>User_0</th>\n      <th>User_1</th>\n      <th>User_2</th>\n      <th>User_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.592492</td>\n      <td>-0.628348</td>\n      <td>-0.961159</td>\n      <td>0.055905</td>\n      <td>0.463966</td>\n      <td>0.243742</td>\n      <td>-1.048130</td>\n      <td>-0.492935</td>\n      <td>-1.039009</td>\n      <td>-0.724031</td>\n      <td>...</td>\n      <td>-0.436216</td>\n      <td>-0.408949</td>\n      <td>-0.407029</td>\n      <td>-0.490410</td>\n      <td>-0.536367</td>\n      <td>-0.539566</td>\n      <td>-0.574343</td>\n      <td>-0.578735</td>\n      <td>-0.575870</td>\n      <td>1.722792</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.215542</td>\n      <td>-1.060438</td>\n      <td>-0.687275</td>\n      <td>0.911497</td>\n      <td>0.266495</td>\n      <td>-0.045726</td>\n      <td>-0.996263</td>\n      <td>-1.073110</td>\n      <td>-1.595935</td>\n      <td>-1.789505</td>\n      <td>...</td>\n      <td>2.577270</td>\n      <td>2.173543</td>\n      <td>3.159511</td>\n      <td>3.022871</td>\n      <td>0.657025</td>\n      <td>2.073117</td>\n      <td>1.741120</td>\n      <td>-0.578735</td>\n      <td>-0.575870</td>\n      <td>-0.580453</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.217034</td>\n      <td>0.061461</td>\n      <td>-0.302453</td>\n      <td>-0.276674</td>\n      <td>-0.609120</td>\n      <td>-0.737363</td>\n      <td>-1.144599</td>\n      <td>-1.598413</td>\n      <td>-1.092270</td>\n      <td>-1.223951</td>\n      <td>...</td>\n      <td>-0.400908</td>\n      <td>-0.385079</td>\n      <td>-0.267008</td>\n      <td>-0.348359</td>\n      <td>-0.541364</td>\n      <td>-0.569416</td>\n      <td>-0.574343</td>\n      <td>-0.578735</td>\n      <td>-0.575870</td>\n      <td>1.722792</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.636827</td>\n      <td>3.089887</td>\n      <td>0.715433</td>\n      <td>1.688836</td>\n      <td>1.115520</td>\n      <td>0.633049</td>\n      <td>-0.753087</td>\n      <td>-0.640998</td>\n      <td>0.643631</td>\n      <td>0.970393</td>\n      <td>...</td>\n      <td>0.141047</td>\n      <td>-0.008948</td>\n      <td>-0.427868</td>\n      <td>-0.301202</td>\n      <td>-0.505539</td>\n      <td>-0.522675</td>\n      <td>-0.574343</td>\n      <td>-0.578735</td>\n      <td>-0.575870</td>\n      <td>1.722792</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.012351</td>\n      <td>0.039718</td>\n      <td>-0.098249</td>\n      <td>-0.213172</td>\n      <td>0.948769</td>\n      <td>-0.058219</td>\n      <td>-0.254097</td>\n      <td>-0.487547</td>\n      <td>0.332959</td>\n      <td>0.028684</td>\n      <td>...</td>\n      <td>-0.565201</td>\n      <td>-0.479292</td>\n      <td>-0.372068</td>\n      <td>-0.497185</td>\n      <td>-0.523806</td>\n      <td>-0.525149</td>\n      <td>-0.574343</td>\n      <td>1.727908</td>\n      <td>-0.575870</td>\n      <td>-0.580453</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8058</th>\n      <td>0.618517</td>\n      <td>1.150023</td>\n      <td>-0.957440</td>\n      <td>-0.735247</td>\n      <td>0.634060</td>\n      <td>1.144167</td>\n      <td>1.837545</td>\n      <td>2.507360</td>\n      <td>1.076999</td>\n      <td>1.533723</td>\n      <td>...</td>\n      <td>-0.278668</td>\n      <td>0.153400</td>\n      <td>-0.380571</td>\n      <td>-0.152910</td>\n      <td>-0.500346</td>\n      <td>-0.453114</td>\n      <td>-0.574343</td>\n      <td>-0.578735</td>\n      <td>1.736502</td>\n      <td>-0.580453</td>\n    </tr>\n    <tr>\n      <th>8059</th>\n      <td>-0.157839</td>\n      <td>-0.326705</td>\n      <td>-0.099050</td>\n      <td>0.460753</td>\n      <td>-0.022372</td>\n      <td>-0.179817</td>\n      <td>1.098608</td>\n      <td>0.432161</td>\n      <td>-0.522874</td>\n      <td>-0.840956</td>\n      <td>...</td>\n      <td>2.779410</td>\n      <td>1.612038</td>\n      <td>1.694883</td>\n      <td>4.498588</td>\n      <td>3.159293</td>\n      <td>2.567176</td>\n      <td>1.741120</td>\n      <td>-0.578735</td>\n      <td>-0.575870</td>\n      <td>-0.580453</td>\n    </tr>\n    <tr>\n      <th>8060</th>\n      <td>0.166508</td>\n      <td>0.010035</td>\n      <td>0.018053</td>\n      <td>-0.268545</td>\n      <td>-1.090114</td>\n      <td>0.455020</td>\n      <td>0.757639</td>\n      <td>1.987062</td>\n      <td>0.999285</td>\n      <td>0.692423</td>\n      <td>...</td>\n      <td>-0.430248</td>\n      <td>-0.416324</td>\n      <td>-0.096056</td>\n      <td>-0.372181</td>\n      <td>-0.508712</td>\n      <td>-0.493789</td>\n      <td>-0.574343</td>\n      <td>-0.578735</td>\n      <td>1.736502</td>\n      <td>-0.580453</td>\n    </tr>\n    <tr>\n      <th>8061</th>\n      <td>0.202509</td>\n      <td>-0.017317</td>\n      <td>-0.267921</td>\n      <td>-0.589426</td>\n      <td>0.478543</td>\n      <td>-0.449163</td>\n      <td>-0.196672</td>\n      <td>-0.763750</td>\n      <td>-0.557585</td>\n      <td>-0.719922</td>\n      <td>...</td>\n      <td>-0.238893</td>\n      <td>-0.510057</td>\n      <td>-0.441859</td>\n      <td>-0.499372</td>\n      <td>-0.511246</td>\n      <td>-0.538075</td>\n      <td>-0.574343</td>\n      <td>1.727908</td>\n      <td>-0.575870</td>\n      <td>-0.580453</td>\n    </tr>\n    <tr>\n      <th>8062</th>\n      <td>-0.271889</td>\n      <td>-0.369114</td>\n      <td>-0.112442</td>\n      <td>0.388119</td>\n      <td>-0.873026</td>\n      <td>0.282208</td>\n      <td>0.973119</td>\n      <td>1.484744</td>\n      <td>0.914695</td>\n      <td>1.269811</td>\n      <td>...</td>\n      <td>0.145982</td>\n      <td>-0.239209</td>\n      <td>-0.348275</td>\n      <td>-0.378249</td>\n      <td>-0.550461</td>\n      <td>-0.559248</td>\n      <td>-0.574343</td>\n      <td>-0.578735</td>\n      <td>1.736502</td>\n      <td>-0.580453</td>\n    </tr>\n  </tbody>\n</table>\n<p>8063 rows × 116 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.514885Z","iopub.execute_input":"2024-02-23T21:32:14.515247Z","iopub.status.idle":"2024-02-23T21:32:14.521870Z","shell.execute_reply.started":"2024-02-23T21:32:14.515219Z","shell.execute_reply":"2024-02-23T21:32:14.520920Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"8701     1.0\n4335     1.0\n11203    1.0\n11448    0.0\n374      2.0\n        ... \n9785     1.0\n7763     2.0\n5218     1.0\n1346     1.0\n3582     2.0\nName: Class, Length: 8063, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.522883Z","iopub.execute_input":"2024-02-23T21:32:14.523259Z","iopub.status.idle":"2024-02-23T21:32:14.536567Z","shell.execute_reply.started":"2024-02-23T21:32:14.523233Z","shell.execute_reply":"2024-02-23T21:32:14.535521Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"2.0    2716\n0.0    2698\n1.0    2649\nName: Class, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"class_model = build_model(num_classes=3)\n\nclass_history = class_model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=50,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:14.537949Z","iopub.execute_input":"2024-02-23T21:32:14.538311Z","iopub.status.idle":"2024-02-23T21:32:19.582393Z","shell.execute_reply.started":"2024-02-23T21:32:14.538283Z","shell.execute_reply":"2024-02-23T21:32:19.581355Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/50\n202/202 [==============================] - 0s 2ms/step - loss: 1.0738 - accuracy: 0.4236 - val_loss: 1.0162 - val_accuracy: 0.4526\nEpoch 2/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.9787 - accuracy: 0.5057 - val_loss: 1.0132 - val_accuracy: 0.4923\nEpoch 3/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.9343 - accuracy: 0.5468 - val_loss: 0.9806 - val_accuracy: 0.5139\nEpoch 4/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.8789 - accuracy: 0.5800 - val_loss: 0.9303 - val_accuracy: 0.5468\nEpoch 5/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.8313 - accuracy: 0.6087 - val_loss: 0.9023 - val_accuracy: 0.5741\nEpoch 6/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.7875 - accuracy: 0.6338 - val_loss: 0.8923 - val_accuracy: 0.5834\nEpoch 7/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.7427 - accuracy: 0.6681 - val_loss: 0.8673 - val_accuracy: 0.5902\nEpoch 8/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.7166 - accuracy: 0.6890 - val_loss: 0.8276 - val_accuracy: 0.6218\nEpoch 9/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.6639 - accuracy: 0.7119 - val_loss: 0.8357 - val_accuracy: 0.6218\nEpoch 10/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.7329 - val_loss: 0.8432 - val_accuracy: 0.6212\nEpoch 11/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.5900 - accuracy: 0.7546 - val_loss: 0.8119 - val_accuracy: 0.6485\nEpoch 12/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.5549 - accuracy: 0.7671 - val_loss: 0.8561 - val_accuracy: 0.6262\nEpoch 13/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7898 - val_loss: 0.7927 - val_accuracy: 0.6609\nEpoch 14/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.8045 - val_loss: 0.8287 - val_accuracy: 0.6497\nEpoch 15/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8186 - val_loss: 0.8322 - val_accuracy: 0.6516\nEpoch 16/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.4174 - accuracy: 0.8395 - val_loss: 0.8339 - val_accuracy: 0.6671\n","output_type":"stream"}]},{"cell_type":"code","source":"class_acc = class_model.evaluate(X_test, y_test, verbose=0)[1]\nprint(\"Test Accuracy (Class Model): {:.2f}%\".format(class_acc * 100))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:19.583513Z","iopub.execute_input":"2024-02-23T21:32:19.583756Z","iopub.status.idle":"2024-02-23T21:32:19.698709Z","shell.execute_reply.started":"2024-02-23T21:32:19.583729Z","shell.execute_reply":"2024-02-23T21:32:19.697587Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Test Accuracy (Class Model): 62.51%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Predicting User","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = preprocess_inputs(data, target='User')","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:19.700133Z","iopub.execute_input":"2024-02-23T21:32:19.700481Z","iopub.status.idle":"2024-02-23T21:32:19.751015Z","shell.execute_reply.started":"2024-02-23T21:32:19.700444Z","shell.execute_reply":"2024-02-23T21:32:19.750115Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:19.752423Z","iopub.execute_input":"2024-02-23T21:32:19.752782Z","iopub.status.idle":"2024-02-23T21:32:19.779770Z","shell.execute_reply.started":"2024-02-23T21:32:19.752745Z","shell.execute_reply":"2024-02-23T21:32:19.779095Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      AF3 delta std  AF3 delta m  AF3 theta std  AF3 theta m  AF3 alpha std  \\\n0         -0.592492    -0.628348      -0.961159     0.055905       0.463966   \n1         -1.215542    -1.060438      -0.687275     0.911497       0.266495   \n2          0.217034     0.061461      -0.302453    -0.276674      -0.609120   \n3          2.636827     3.089887       0.715433     1.688836       1.115520   \n4         -0.012351     0.039718      -0.098249    -0.213172       0.948769   \n...             ...          ...            ...          ...            ...   \n8058       0.618517     1.150023      -0.957440    -0.735247       0.634060   \n8059      -0.157839    -0.326705      -0.099050     0.460753      -0.022372   \n8060       0.166508     0.010035       0.018053    -0.268545      -1.090114   \n8061       0.202509    -0.017317      -0.267921    -0.589426       0.478543   \n8062      -0.271889    -0.369114      -0.112442     0.388119      -0.873026   \n\n      AF3 alpha m  AF3 beta std  AF3 beta m  F7 delta std  F7 delta m  ...  \\\n0        0.243742     -1.048130   -0.492935     -1.039009   -0.724031  ...   \n1       -0.045726     -0.996263   -1.073110     -1.595935   -1.789505  ...   \n2       -0.737363     -1.144599   -1.598413     -1.092270   -1.223951  ...   \n3        0.633049     -0.753087   -0.640998      0.643631    0.970393  ...   \n4       -0.058219     -0.254097   -0.487547      0.332959    0.028684  ...   \n...           ...           ...         ...           ...         ...  ...   \n8058     1.144167      1.837545    2.507360      1.076999    1.533723  ...   \n8059    -0.179817      1.098608    0.432161     -0.522874   -0.840956  ...   \n8060     0.455020      0.757639    1.987062      0.999285    0.692423  ...   \n8061    -0.449163     -0.196672   -0.763750     -0.557585   -0.719922  ...   \n8062     0.282208      0.973119    1.484744      0.914695    1.269811  ...   \n\n      AF4 delta m  AF4 theta std  AF4 theta m  AF4 alpha std  AF4 alpha m  \\\n0       -0.535664      -0.436216    -0.408949      -0.407029    -0.490410   \n1        3.858153       2.577270     2.173543       3.159511     3.022871   \n2       -0.537446      -0.400908    -0.385079      -0.267008    -0.348359   \n3       -0.382336       0.141047    -0.008948      -0.427868    -0.301202   \n4       -0.465464      -0.565201    -0.479292      -0.372068    -0.497185   \n...           ...            ...          ...            ...          ...   \n8058     1.371221      -0.278668     0.153400      -0.380571    -0.152910   \n8059     1.627338       2.779410     1.612038       1.694883     4.498588   \n8060    -0.167387      -0.430248    -0.416324      -0.096056    -0.372181   \n8061    -0.463766      -0.238893    -0.510057      -0.441859    -0.499372   \n8062    -0.061804       0.145982    -0.239209      -0.348275    -0.378249   \n\n      AF4 beta std  AF4 beta m  Class_0.0  Class_1.0  Class_2.0  \n0        -0.536367   -0.539566  -0.709147   1.429612  -0.712705  \n1         0.657025    2.073117  -0.709147   1.429612  -0.712705  \n2        -0.541364   -0.569416  -0.709147   1.429612  -0.712705  \n3        -0.505539   -0.522675   1.410145  -0.699491  -0.712705  \n4        -0.523806   -0.525149  -0.709147  -0.699491   1.403105  \n...            ...         ...        ...        ...        ...  \n8058     -0.500346   -0.453114  -0.709147   1.429612  -0.712705  \n8059      3.159293    2.567176  -0.709147  -0.699491   1.403105  \n8060     -0.508712   -0.493789  -0.709147   1.429612  -0.712705  \n8061     -0.511246   -0.538075  -0.709147   1.429612  -0.712705  \n8062     -0.550461   -0.559248  -0.709147  -0.699491   1.403105  \n\n[8063 rows x 115 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AF3 delta std</th>\n      <th>AF3 delta m</th>\n      <th>AF3 theta std</th>\n      <th>AF3 theta m</th>\n      <th>AF3 alpha std</th>\n      <th>AF3 alpha m</th>\n      <th>AF3 beta std</th>\n      <th>AF3 beta m</th>\n      <th>F7 delta std</th>\n      <th>F7 delta m</th>\n      <th>...</th>\n      <th>AF4 delta m</th>\n      <th>AF4 theta std</th>\n      <th>AF4 theta m</th>\n      <th>AF4 alpha std</th>\n      <th>AF4 alpha m</th>\n      <th>AF4 beta std</th>\n      <th>AF4 beta m</th>\n      <th>Class_0.0</th>\n      <th>Class_1.0</th>\n      <th>Class_2.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.592492</td>\n      <td>-0.628348</td>\n      <td>-0.961159</td>\n      <td>0.055905</td>\n      <td>0.463966</td>\n      <td>0.243742</td>\n      <td>-1.048130</td>\n      <td>-0.492935</td>\n      <td>-1.039009</td>\n      <td>-0.724031</td>\n      <td>...</td>\n      <td>-0.535664</td>\n      <td>-0.436216</td>\n      <td>-0.408949</td>\n      <td>-0.407029</td>\n      <td>-0.490410</td>\n      <td>-0.536367</td>\n      <td>-0.539566</td>\n      <td>-0.709147</td>\n      <td>1.429612</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.215542</td>\n      <td>-1.060438</td>\n      <td>-0.687275</td>\n      <td>0.911497</td>\n      <td>0.266495</td>\n      <td>-0.045726</td>\n      <td>-0.996263</td>\n      <td>-1.073110</td>\n      <td>-1.595935</td>\n      <td>-1.789505</td>\n      <td>...</td>\n      <td>3.858153</td>\n      <td>2.577270</td>\n      <td>2.173543</td>\n      <td>3.159511</td>\n      <td>3.022871</td>\n      <td>0.657025</td>\n      <td>2.073117</td>\n      <td>-0.709147</td>\n      <td>1.429612</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.217034</td>\n      <td>0.061461</td>\n      <td>-0.302453</td>\n      <td>-0.276674</td>\n      <td>-0.609120</td>\n      <td>-0.737363</td>\n      <td>-1.144599</td>\n      <td>-1.598413</td>\n      <td>-1.092270</td>\n      <td>-1.223951</td>\n      <td>...</td>\n      <td>-0.537446</td>\n      <td>-0.400908</td>\n      <td>-0.385079</td>\n      <td>-0.267008</td>\n      <td>-0.348359</td>\n      <td>-0.541364</td>\n      <td>-0.569416</td>\n      <td>-0.709147</td>\n      <td>1.429612</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.636827</td>\n      <td>3.089887</td>\n      <td>0.715433</td>\n      <td>1.688836</td>\n      <td>1.115520</td>\n      <td>0.633049</td>\n      <td>-0.753087</td>\n      <td>-0.640998</td>\n      <td>0.643631</td>\n      <td>0.970393</td>\n      <td>...</td>\n      <td>-0.382336</td>\n      <td>0.141047</td>\n      <td>-0.008948</td>\n      <td>-0.427868</td>\n      <td>-0.301202</td>\n      <td>-0.505539</td>\n      <td>-0.522675</td>\n      <td>1.410145</td>\n      <td>-0.699491</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.012351</td>\n      <td>0.039718</td>\n      <td>-0.098249</td>\n      <td>-0.213172</td>\n      <td>0.948769</td>\n      <td>-0.058219</td>\n      <td>-0.254097</td>\n      <td>-0.487547</td>\n      <td>0.332959</td>\n      <td>0.028684</td>\n      <td>...</td>\n      <td>-0.465464</td>\n      <td>-0.565201</td>\n      <td>-0.479292</td>\n      <td>-0.372068</td>\n      <td>-0.497185</td>\n      <td>-0.523806</td>\n      <td>-0.525149</td>\n      <td>-0.709147</td>\n      <td>-0.699491</td>\n      <td>1.403105</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8058</th>\n      <td>0.618517</td>\n      <td>1.150023</td>\n      <td>-0.957440</td>\n      <td>-0.735247</td>\n      <td>0.634060</td>\n      <td>1.144167</td>\n      <td>1.837545</td>\n      <td>2.507360</td>\n      <td>1.076999</td>\n      <td>1.533723</td>\n      <td>...</td>\n      <td>1.371221</td>\n      <td>-0.278668</td>\n      <td>0.153400</td>\n      <td>-0.380571</td>\n      <td>-0.152910</td>\n      <td>-0.500346</td>\n      <td>-0.453114</td>\n      <td>-0.709147</td>\n      <td>1.429612</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>8059</th>\n      <td>-0.157839</td>\n      <td>-0.326705</td>\n      <td>-0.099050</td>\n      <td>0.460753</td>\n      <td>-0.022372</td>\n      <td>-0.179817</td>\n      <td>1.098608</td>\n      <td>0.432161</td>\n      <td>-0.522874</td>\n      <td>-0.840956</td>\n      <td>...</td>\n      <td>1.627338</td>\n      <td>2.779410</td>\n      <td>1.612038</td>\n      <td>1.694883</td>\n      <td>4.498588</td>\n      <td>3.159293</td>\n      <td>2.567176</td>\n      <td>-0.709147</td>\n      <td>-0.699491</td>\n      <td>1.403105</td>\n    </tr>\n    <tr>\n      <th>8060</th>\n      <td>0.166508</td>\n      <td>0.010035</td>\n      <td>0.018053</td>\n      <td>-0.268545</td>\n      <td>-1.090114</td>\n      <td>0.455020</td>\n      <td>0.757639</td>\n      <td>1.987062</td>\n      <td>0.999285</td>\n      <td>0.692423</td>\n      <td>...</td>\n      <td>-0.167387</td>\n      <td>-0.430248</td>\n      <td>-0.416324</td>\n      <td>-0.096056</td>\n      <td>-0.372181</td>\n      <td>-0.508712</td>\n      <td>-0.493789</td>\n      <td>-0.709147</td>\n      <td>1.429612</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>8061</th>\n      <td>0.202509</td>\n      <td>-0.017317</td>\n      <td>-0.267921</td>\n      <td>-0.589426</td>\n      <td>0.478543</td>\n      <td>-0.449163</td>\n      <td>-0.196672</td>\n      <td>-0.763750</td>\n      <td>-0.557585</td>\n      <td>-0.719922</td>\n      <td>...</td>\n      <td>-0.463766</td>\n      <td>-0.238893</td>\n      <td>-0.510057</td>\n      <td>-0.441859</td>\n      <td>-0.499372</td>\n      <td>-0.511246</td>\n      <td>-0.538075</td>\n      <td>-0.709147</td>\n      <td>1.429612</td>\n      <td>-0.712705</td>\n    </tr>\n    <tr>\n      <th>8062</th>\n      <td>-0.271889</td>\n      <td>-0.369114</td>\n      <td>-0.112442</td>\n      <td>0.388119</td>\n      <td>-0.873026</td>\n      <td>0.282208</td>\n      <td>0.973119</td>\n      <td>1.484744</td>\n      <td>0.914695</td>\n      <td>1.269811</td>\n      <td>...</td>\n      <td>-0.061804</td>\n      <td>0.145982</td>\n      <td>-0.239209</td>\n      <td>-0.348275</td>\n      <td>-0.378249</td>\n      <td>-0.550461</td>\n      <td>-0.559248</td>\n      <td>-0.709147</td>\n      <td>-0.699491</td>\n      <td>1.403105</td>\n    </tr>\n  </tbody>\n</table>\n<p>8063 rows × 115 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:19.781038Z","iopub.execute_input":"2024-02-23T21:32:19.781273Z","iopub.status.idle":"2024-02-23T21:32:19.790733Z","shell.execute_reply.started":"2024-02-23T21:32:19.781249Z","shell.execute_reply":"2024-02-23T21:32:19.788941Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"8701     3\n4335     0\n11203    3\n11448    3\n374      1\n        ..\n9785     2\n7763     0\n5218     2\n1346     1\n3582     2\nName: User, Length: 8063, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:19.792113Z","iopub.execute_input":"2024-02-23T21:32:19.792485Z","iopub.status.idle":"2024-02-23T21:32:19.803713Z","shell.execute_reply.started":"2024-02-23T21:32:19.792456Z","shell.execute_reply":"2024-02-23T21:32:19.802788Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"3    2032\n1    2023\n2    2008\n0    2000\nName: User, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"user_model = build_model(num_classes=4)\n\nuser_history = user_model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=50,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:19.804852Z","iopub.execute_input":"2024-02-23T21:32:19.805117Z","iopub.status.idle":"2024-02-23T21:32:29.083233Z","shell.execute_reply.started":"2024-02-23T21:32:19.805092Z","shell.execute_reply":"2024-02-23T21:32:29.081988Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/50\n202/202 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.9591 - val_loss: 0.0240 - val_accuracy: 0.9944\nEpoch 2/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.0108 - val_accuracy: 0.9963\nEpoch 3/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0052 - val_accuracy: 0.9981\nEpoch 4/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.0042 - val_accuracy: 0.9994\nEpoch 5/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0042 - val_accuracy: 0.9975\nEpoch 6/50\n202/202 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0019 - val_accuracy: 0.9994\nEpoch 7/50\n202/202 [==============================] - 0s 1ms/step - loss: 8.0638e-04 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 0.9981\nEpoch 8/50\n202/202 [==============================] - 0s 1ms/step - loss: 3.1247e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\nEpoch 9/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.9977e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 0.9994\nEpoch 10/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.5349e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\nEpoch 11/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.2242e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\nEpoch 12/50\n202/202 [==============================] - 0s 1ms/step - loss: 9.3170e-05 - accuracy: 1.0000 - val_loss: 8.2909e-04 - val_accuracy: 1.0000\nEpoch 13/50\n202/202 [==============================] - 0s 1ms/step - loss: 8.3133e-05 - accuracy: 1.0000 - val_loss: 9.2969e-04 - val_accuracy: 1.0000\nEpoch 14/50\n202/202 [==============================] - 0s 1ms/step - loss: 7.2952e-05 - accuracy: 1.0000 - val_loss: 7.7195e-04 - val_accuracy: 1.0000\nEpoch 15/50\n202/202 [==============================] - 0s 1ms/step - loss: 6.5184e-05 - accuracy: 1.0000 - val_loss: 7.7609e-04 - val_accuracy: 1.0000\nEpoch 16/50\n202/202 [==============================] - 0s 1ms/step - loss: 4.9039e-05 - accuracy: 1.0000 - val_loss: 6.9583e-04 - val_accuracy: 1.0000\nEpoch 17/50\n202/202 [==============================] - 0s 1ms/step - loss: 4.3611e-05 - accuracy: 1.0000 - val_loss: 6.9421e-04 - val_accuracy: 1.0000\nEpoch 18/50\n202/202 [==============================] - 0s 1ms/step - loss: 3.9717e-05 - accuracy: 1.0000 - val_loss: 6.3895e-04 - val_accuracy: 1.0000\nEpoch 19/50\n202/202 [==============================] - 0s 1ms/step - loss: 3.0554e-05 - accuracy: 1.0000 - val_loss: 6.4138e-04 - val_accuracy: 1.0000\nEpoch 20/50\n202/202 [==============================] - 0s 1ms/step - loss: 2.7416e-05 - accuracy: 1.0000 - val_loss: 5.5671e-04 - val_accuracy: 1.0000\nEpoch 21/50\n202/202 [==============================] - 0s 1ms/step - loss: 2.3704e-05 - accuracy: 1.0000 - val_loss: 6.8206e-04 - val_accuracy: 1.0000\nEpoch 22/50\n202/202 [==============================] - 0s 1ms/step - loss: 2.0238e-05 - accuracy: 1.0000 - val_loss: 4.9777e-04 - val_accuracy: 1.0000\nEpoch 23/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.8751e-05 - accuracy: 1.0000 - val_loss: 4.5096e-04 - val_accuracy: 1.0000\nEpoch 24/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.6306e-05 - accuracy: 1.0000 - val_loss: 3.9153e-04 - val_accuracy: 1.0000\nEpoch 25/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.4601e-05 - accuracy: 1.0000 - val_loss: 4.8903e-04 - val_accuracy: 1.0000\nEpoch 26/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.2927e-05 - accuracy: 1.0000 - val_loss: 5.0263e-04 - val_accuracy: 1.0000\nEpoch 27/50\n202/202 [==============================] - 0s 1ms/step - loss: 1.0820e-05 - accuracy: 1.0000 - val_loss: 3.2323e-04 - val_accuracy: 1.0000\nEpoch 28/50\n202/202 [==============================] - 0s 1ms/step - loss: 9.4752e-06 - accuracy: 1.0000 - val_loss: 6.5167e-04 - val_accuracy: 1.0000\nEpoch 29/50\n202/202 [==============================] - 0s 1ms/step - loss: 7.7459e-06 - accuracy: 1.0000 - val_loss: 3.2495e-04 - val_accuracy: 1.0000\nEpoch 30/50\n202/202 [==============================] - 0s 1ms/step - loss: 7.0005e-06 - accuracy: 1.0000 - val_loss: 3.0629e-04 - val_accuracy: 1.0000\nEpoch 31/50\n202/202 [==============================] - 0s 1ms/step - loss: 6.1137e-06 - accuracy: 1.0000 - val_loss: 3.5605e-04 - val_accuracy: 1.0000\nEpoch 32/50\n202/202 [==============================] - 0s 1ms/step - loss: 5.4758e-06 - accuracy: 1.0000 - val_loss: 3.8958e-04 - val_accuracy: 1.0000\nEpoch 33/50\n202/202 [==============================] - 0s 1ms/step - loss: 4.8451e-06 - accuracy: 1.0000 - val_loss: 3.0960e-04 - val_accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"user_acc = user_model.evaluate(X_test, y_test, verbose=0)[1]\nprint(\"Test Accuracy (User Model): {:.2f}%\".format(user_acc * 100))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T21:32:29.084667Z","iopub.execute_input":"2024-02-23T21:32:29.084925Z","iopub.status.idle":"2024-02-23T21:32:29.197340Z","shell.execute_reply.started":"2024-02-23T21:32:29.084899Z","shell.execute_reply":"2024-02-23T21:32:29.196188Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Test Accuracy (User Model): 99.97%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/hEExwYfoieY","metadata":{}}]}